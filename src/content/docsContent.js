export const MODELS_MARKDOWN = '# Neural Network Architecture 3D Visualization Guides\n\n## Table of Contents\n1. [Convolutional Neural Networks (CNNs)](#cnn-guide)\n2. [Mixture of Experts (MoE)](#moe-guide)\n3. [YOLO Object Detection](#yolo-guide)\n4. [Animation Techniques](#animation-techniques)\n\n---\n\n## CNN Guide {#cnn-guide}\n\n### Architecture Overview\nCNN process images through hierarchical feature extraction using convolution, pooling, and fully connected layers.\n\n### 3D Visualization Structure\n\n#### Layer Components:\n```JavaScript\n// Input Image Layer\nPosition: (0, -10, 0)\nGeometry: BoxGeometry(width, height, 3) // RGB channels\nMaterial: TextureLoader for actual image display\nAnimation: Rotate slowly to show 3D nature\nLabel: "Input Image (H×W×3)"\n\n// Convolutional Layers\nPosition: (0, y_level, 0) where y_level increases by 4\nGeometry: Multiple BoxGeometry representing feature maps\n- Size decreases with depth (width/height)\n- Depth increases (more channels)\nMaterial: Different colors for different channels\nAnimation: Sliding kernel effect, feature activation waves\nLabel: "Conv Layer (filters×H×W)"\n\n// Pooling Layers  \nPosition: Between conv layers\nGeometry: Smaller boxes showing downsampling\nMaterial: Semi-transparent blue\nAnimation: "Shrinking" effect to show dimension reduction\nLabel: "Max/Avg Pool (H/2×W/2)"\n\n// Fully Connected Layers\nPosition: (0, 8, 0)\nGeometry: Linear arrangement of spheres\nMaterial: Gradient from blue to green\nAnimation: Neuron activation pulses\nLabel: "FC Layer (N neurons)"\n```\n\n#### Key Visualizations:\n\n**1. Convolution Operation:**\n```javascript\n// 3D Kernel sliding across feature map\nconst kernelGeometry = new THREE.BoxGeometry(0.5, 0.5, 0.3);\nconst kernelMaterial = new THREE.MeshLambertMaterial({\n    color: 0xff0000,\n    transparent: true,\n    opacity: 0.7\n});\n\n// Animation: Kernel slides across input\nfunction animateConvolution(kernel, inputLayer) {\n    const positions = generateSlidingPositions(inputLayer);\n    // GSAP timeline for smooth sliding motion\n    const timeline = gsap.timeline({repeat: -1});\n    positions.forEach((pos, i) => {\n        timeline.to(kernel.position, {\n            duration: 0.2,\n            x: pos.x,\n            y: pos.y,\n            ease: "none"\n        });\n    });\n}\n```\n\n**2. Feature Map Activation:**\n```JavaScript\n// Show how different filters detect different features\nfunction visualizeFeatureActivation(featureMap, filterType) {\n    const activationIntensity = calculateActivation(filterType);\n    \n    // Color coding for different features\n    const colors = {\n        \'edges\': 0x00ff00,\n        \'corners\': 0xff0000,\n        \'textures\': 0x0000ff\n    };\n    \n    // Animate based on feature strength\n    gsap.to(featureMap.material, {\n        color: colors[filterType],\n        emissiveIntensity: activationIntensity,\n        duration: 0.5,\n        repeat: -1,\n        yoyo: true\n    });\n}\n```\n\n**3. Pooling Visualization:**\n```javascript\n// Show max pooling selection\nfunction animateMaxPooling(inputRegion, outputNeuron) {\n    // Highlight maximum value in region\n    const maxNeuron = findMaxInRegion(inputRegion);\n    \n    // Create connection line\n    const connectionGeometry = new THREE.BufferGeometry().setFromPoints([\n        maxNeuron.position,\n        outputNeuron.position\n    ]);\n    \n    // Animate value transfer\n    createFlowingParticle(maxNeuron.position, outputNeuron.position);\n}\n```\n\n### Training Process Animation:\n1. **Forward Pass**: Show data flowing through layers\n2. **Feature Detection**: Highlight activated features\n3. **Pooling**: Animate dimension reduction\n4. **Classification**: Show final decision process\n\n---\n\n## MOE Guide {#moe-guide}\n\n### Architecture Overview\nMoE uses multiple specialized expert networks with a gating mechanism that routes tokens to the most relevant experts, enabling larger model capacity without proportional computational cost.\n\n### 3D Visualization Structure:\n\n#### Expert Networks Layout:\n```javascript\n// Circular arrangement of experts\nconst numExperts = 8;\nconst radius = 5;\n\nfor (let i = 0; i < numExperts; i++) {\n    const angle = (i / numExperts) * Math.PI * 2;\n    const expertPos = new THREE.Vector3(\n        Math.cos(angle) * radius,\n        0,\n        Math.sin(angle) * radius\n    );\n    \n    // Expert Network\n    const expertGeometry = new THREE.CylinderGeometry(0.8, 0.8, 2, 8);\n    const expertMaterial = new THREE.MeshLambertMaterial({\n        color: getExpertColor(i),\n        transparent: true,\n        opacity: 0.3 // Initially inactive\n    });\n    \n    const expert = new THREE.Mesh(expertGeometry, expertMaterial);\n    expert.position.copy(expertPos);\n    expert.userData = {\n        type: \'expert\',\n        id: i,\n        specialization: getExpertSpecialization(i),\n        activity: 0\n    };\n    \n    scene.add(expert);\n}\n\n// Central Gating Network\nconst gateGeometry = new THREE.SphereGeometry(1, 16, 12);\nconst gateMaterial = new THREE.MeshLambertMaterial({\n    color: 0xFFD700,\n    emissive: 0x444400\n});\nconst gate = new THREE.Mesh(gateGeometry, gateMaterial);\ngate.position.set(0, 0, 0);\ngate.userData = { type: \'gate\' };\nscene.add(gate);\n```\n\n#### Token Routing Visualization:\n```javascript\nfunction animateTokenRouting(token, gatingScores) {\n    const timeline = gsap.timeline();\n    \n    // 1. Token arrives at gate\n    timeline.to(token.position, {\n        x: 0, y: 2, z: 0,\n        duration: 1,\n        ease: "power2.out"\n    });\n    \n    // 2. Gating network computes scores\n    timeline.call(() => {\n        animateGatingComputation(gate, gatingScores);\n    });\n    \n    // 3. Route to top-k experts\n    const topKExperts = getTopKExperts(gatingScores, 2);\n    \n    topKExperts.forEach((expertId, index) => {\n        timeline.call(() => {\n            routeToExpert(token.clone(), expertId, gatingScores[expertId]);\n        }, null, 2 + index * 0.5);\n    });\n}\n\nfunction routeToExpert(token, expertId, weight) {\n    const expert = getExpertById(expertId);\n    \n    // Activate expert based on routing weight\n    gsap.to(expert.material, {\n        opacity: 0.3 + (weight * 0.7),\n        emissiveIntensity: weight * 0.5,\n        duration: 0.5\n    });\n    \n    // Create routing line\n    const routingLine = createRoutingLine(gate.position, expert.position, weight);\n    \n    // Animate token to expert\n    gsap.to(token.position, {\n        x: expert.position.x,\n        y: expert.position.y + 1.5,\n        z: expert.position.z,\n        duration: 1,\n        ease: "power2.inOut"\n    });\n    \n    // Process at expert\n    setTimeout(() => {\n        animateExpertProcessing(expert, token);\n    }, 1000);\n}\n\nfunction animateExpertProcessing(expert, token) {\n    // Expert processing animation\n    const originalScale = expert.scale.clone();\n    \n    gsap.to(expert.scale, {\n        x: 1.2, y: 1.2, z: 1.2,\n        duration: 0.3,\n        yoyo: true,\n        repeat: 1,\n        onComplete: () => {\n            // Send processed token back\n            animateTokenReturn(token, expert);\n        }\n    });\n}\n```\n\n#### Load Balancing Visualization:\n```javascript\nfunction visualizeLoadBalancing(expertUsage) {\n    experts.forEach((expert, i) => {\n        const usage = expertUsage[i];\n        const targetHeight = 2 * usage; // Scale height by usage\n        \n        gsap.to(expert.scale, {\n            y: targetHeight,\n            duration: 1,\n            ease: "power2.out"\n        });\n        \n        // Color intensity based on usage\n        const intensity = Math.min(usage * 2, 1);\n        gsap.to(expert.material, {\n            emissiveIntensity: intensity * 0.5,\n            duration: 1\n        });\n    });\n}\n```\n\n---\n\n## YOLO Guide {#yolo-guide}\n\n### Architecture Overview\nYOLO (You Only Look Once) performs object detection in a single forward pass by dividing images into grids and predicting bounding boxes and classes simultaneously.\n\n### 3D Visualization Structure:\n\n#### Grid-Based Detection:\n```javascript\n// Input Image Grid\nconst gridSize = 7; // 7x7 grid for YOLO v1\nconst cellSize = 1;\n\nfor (let i = 0; i < gridSize; i++) {\n    for (let j = 0; j < gridSize; j++) {\n        const cellGeometry = new THREE.PlaneGeometry(cellSize, cellSize);\n        const cellMaterial = new THREE.MeshBasicMaterial({\n            color: 0x444444,\n            transparent: true,\n            opacity: 0.3,\n            side: THREE.DoubleSide\n        });\n        \n        const cell = new THREE.Mesh(cellGeometry, cellMaterial);\n        cell.position.set(\n            i * cellSize - gridSize/2,\n            j * cellSize - gridSize/2,\n            0\n        );\n        \n        cell.userData = {\n            type: \'grid_cell\',\n            gridX: i,\n            gridY: j,\n            confidence: 0,\n            boxes: []\n        };\n        \n        scene.add(cell);\n    }\n}\n\n// Backbone Network (Feature Extraction)\nconst backbonePos = new THREE.Vector3(0, 0, -8);\ncreateCNNBackbone(backbonePos);\n\n// Detection Head\nconst detectionHeadPos = new THREE.Vector3(0, 0, -4);\ncreateDetectionHead(detectionHeadPos);\n```\n\n#### Bounding Box Prediction:\n```javascript\nfunction animateBoundingBoxPrediction(gridCell, predictions) {\n    // Each cell predicts multiple bounding boxes\n    const boxColors = [0xFF0000, 0x00FF00]; // Red and Green for 2 boxes\n    \n    predictions.boxes.forEach((box, index) => {\n        const boxGeometry = new THREE.BoxGeometry(\n            box.width, box.height, 0.1\n        );\n        \n        const boxMaterial = new THREE.MeshBasicMaterial({\n            color: boxColors[index],\n            transparent: true,\n            opacity: box.confidence,\n            wireframe: true\n        });\n        \n        const boxMesh = new THREE.Mesh(boxGeometry, boxMaterial);\n        boxMesh.position.set(\n            gridCell.position.x + box.centerX,\n            gridCell.position.y + box.centerY,\n            gridCell.position.z + 0.1\n        );\n        \n        // Animate confidence with pulsing\n        gsap.to(boxMaterial, {\n            opacity: box.confidence * 0.8,\n            duration: 1,\n            repeat: -1,\n            yoyo: true\n        });\n        \n        scene.add(boxMesh);\n        gridCell.userData.boxes.push(boxMesh);\n    });\n}\n\nfunction animateClassPrediction(gridCell, classPredictions) {\n    // Show class probabilities as colored indicators\n    const classColors = {\n        \'person\': 0xFF0000,\n        \'car\': 0x00FF00,\n        \'dog\': 0x0000FF,\n        \'bicycle\': 0xFFFF00\n    };\n    \n    Object.entries(classPredictions).forEach(([className, probability], index) => {\n        if (probability > 0.1) { // Only show significant predictions\n            const indicator = createClassIndicator(\n                className, \n                probability, \n                classColors[className]\n            );\n            \n            indicator.position.set(\n                gridCell.position.x + (index * 0.2 - 0.3),\n                gridCell.position.y + 0.6,\n                0.2\n            );\n            \n            scene.add(indicator);\n        }\n    });\n}\n```\n\n#### Non-Maximum Suppression:\n```javascript\nfunction animateNMS(detectedBoxes) {\n    const timeline = gsap.timeline();\n    \n    // Phase 1: Show all detections\n    timeline.call(() => {\n        detectedBoxes.forEach(box => {\n            box.material.opacity = box.userData.confidence;\n        });\n    });\n    \n    // Phase 2: Sort boxes by confidence\n    timeline.call(() => {\n        detectedBoxes.sort((a, b) => b.userData.confidence - a.userData.confidence);\n        // Visually reorder or highlight the highest confidence box\n    }, null, "+=1");\n\n    // Phase 3: Iterate and discard overlapping boxes\n    timeline.call(() => {\n        for (let i = 0; i < detectedBoxes.length; i++) {\n            if (detectedBoxes[i].userData.isKept === false) continue;\n            \n            for (let j = i + 1; j < detectedBoxes.length; j++) {\n                if (detectedBoxes[j].userData.isKept === false) continue;\n                \n                const iou = calculateIOU(detectedBoxes[i], detectedBoxes[j]);\n                if (iou > 0.5) { // IoU threshold\n                    // Fade out the lower confidence box\n                    gsap.to(detectedBoxes[j].material, { opacity: 0.1, duration: 0.5 });\n                    detectedBoxes[j].userData.isKept = false;\n                }\n            }\n        }\n    }, null, "+=1");\n\n    // Phase 4: Show final detections\n    timeline.call(() => {\n        detectedBoxes.forEach(box => {\n            if (box.userData.isKept !== false) {\n                // Make final boxes solid and bright\n                gsap.to(box.material, { opacity: 1.0, wireframe: false, duration: 0.5 });\n            } else {\n                // Remove discarded boxes\n                scene.remove(box);\n            }\n        });\n    }, null, "+=1.5");\n}\n```\n\n---\n\n## Animation Techniques {#animation-techniques}\n\n### Overview\nThis section details reusable animation patterns for visualizing neural network operations.\n\n### 1. Data Flow Particles\nRepresent data tokens, gradients, or information moving through the network.\n\n```javascript\nfunction createFlowingParticle(startPos, endPos, color = 0x00ffff, speed = 1) {\n    const particleGeometry = new THREE.SphereGeometry(0.08, 6, 6);\n    const particleMaterial = new THREE.MeshBasicMaterial({\n        color: color,\n        transparent: true\n    });\n    \n    const particle = new THREE.Mesh(particleGeometry, particleMaterial);\n    particle.position.copy(startPos);\n    scene.add(particle);\n    \n    gsap.to(particle.position, {\n        x: endPos.x,\n        y: endPos.y,\n        z: endPos.z,\n        duration: 2 / speed,\n        ease: "power1.inOut",\n        onComplete: () => scene.remove(particle)\n    });\n}\n```\n\n### 2. Activation Visualization\nShow neuron or layer activation using light, color, or scale.\n\n```javascript\nfunction animateActivation(targetObject, intensity, color = 0x00ff00) {\n    // Method 1: Emissive intensity\n    gsap.to(targetObject.material, {\n        emissiveIntensity: intensity,\n        duration: 0.3,\n        yoyo: true,\n        repeat: 1\n    });\n    \n    // Method 2: Scaling (pulsing)\n    const originalScale = targetObject.scale.clone();\n    gsap.to(targetObject.scale, {\n        x: originalScale.x * (1 + intensity * 0.5),\n        y: originalScale.y * (1 + intensity * 0.5),\n        z: originalScale.z * (1 + intensity * 0.5),\n        duration: 0.3,\n        yoyo: true,\n        repeat: 1\n    });\n}\n```\n\n### 3. Weight Update Visualization\nIndicate changes in connection weights during backpropagation.\n\n```javascript\nfunction visualizeWeightUpdate(connectionLine, updateMagnitude) {\n    const originalColor = connectionLine.material.color.clone();\n    const updateColor = updateMagnitude > 0 ? new THREE.Color(0x00ff00) : new THREE.Color(0xff0000);\n    \n    connectionLine.material.color.lerp(updateColor, Math.abs(updateMagnitude));\n    \n    // Fade back to original color\n    gsap.to(connectionLine.material.color, {\n        r: originalColor.r,\n        g: originalColor.g,\n        b: originalColor.b,\n        duration: 1.5\n    });\n}\n```\n\n### 4. Component Highlighting\nDraw attention to the currently active part of the network.\n\n```javascript\nfunction highlightComponent(component, duration = 1) {\n    // Create a highlight outline or halo effect\n    const outlineMaterial = new THREE.MeshBasicMaterial({\n        color: 0xffff00,\n        side: THREE.BackSide\n    });\n    const outlineMesh = new THREE.Mesh(component.geometry, outlineMaterial);\n    outlineMesh.position.copy(component.position);\n    outlineMesh.scale.multiplyScalar(1.1);\n    scene.add(outlineMesh);\n    \n    // Fade out the highlight\n    gsap.to(outlineMesh.material, {\n        opacity: 0,\n        duration: duration,\n        onComplete: () => scene.remove(outlineMesh)\n    });\n}\n```\n\n### 5. Using GSAP for Timelines\nOrchestrate complex, multi-step animations with precision.\n\n```javascript\n// Example of a forward pass animation timeline\nconst forwardPass = gsap.timeline();\n\nforwardPass\n    .addLabel("input")\n    .call(() => animateActivation(inputLayer, 1.0), null, "input")\n    .call(() => createFlowingParticle(inputLayer.position, hiddenLayer1.position), null, "input")\n    \n    .addLabel("hidden1", "+=1")\n    .call(() => animateActivation(hiddenLayer1, 0.8), null, "hidden1")\n    .call(() => createFlowingParticle(hiddenLayer1.position, outputLayer.position), null, "hidden1")\n    \n    .addLabel("output", "+=1")\n    .call(() => animateActivation(outputLayer, 1.0, 0x0000ff), null, "output");\n```\n';

export const SOURCES_MARKDOWN = "# Sources & Lessons – NexaVisualize\n\nThis document compiles all references, citations, and insights used to build **NexaVisualize**.  \nIt serves as both a bibliography and a reflection on the project.  \n\n---\n\n## Citations & References\n\nThe following resources were directly referenced while implementing different architectures in NexaVisualize:\n\n- **Feedforward Neural Networks (FNNs)**  \n  - Goodfellow, Ian, Bengio, Yoshua, Courville, Aaron. *Deep Learning.* MIT Press (2016).\n  - Introduction to Feedforward Neural Networks (FNNs): [Intro to FNN's](https://novarkservices.com/feedforward-neural-networks-fnns-a-fundamental-deep-learning-model/)\n  - Feedforward Neural Networks: The Backbone of Deep Learning: [The bacbone of DL](https://medium.com/@h6364749/feedforward-neural-networks-the-backbone-of-deep-learning-22bfa6635ab7)\n  - Stanford CS231n: [Neural Networks Part 1](http://cs231n.github.io/neural-networks-1/).  \n\n- **Convolutional Neural Networks (CNNs)**  \n  - LeCun, Yann, et al. *Gradient-Based Learning Applied to Document Recognition.* Proceedings of the IEEE (1998).\n  - An Introduction to Convolutional Neural Networks (CNNs): [An intro To CNN's](https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns)\n  - Demystifying CNNs: A Deep Dive into Convolutional Neural Network Fundamentals: [A Deep Dive into CNN's](https://medium.com/@karemsaeed2468/demystifying-cnns-a-deep-dive-into-convolutional-neural-network-fundamentals-0b9ed1d1d2fa)\n  - Datacamp: [Convolutional Neural Networks Explained](https://www.datacamp.com/tutorial/convolutional-neural-networks).  \n\n- **Transformers**  \n  - Vaswani, A., et al. *Attention Is All You Need.* NeurIPS (2017).  \n  - VitalFlux: [Transformer Neural Network Architecture](https://vitalflux.com/transformer-neural-network-architecture-explained/).\n  - Attention is all you need: [Attention is all you need](https://huggingface.co/papers/1706.03762)\n  - How do Transfomers work Huggingface: [How do transformers work](https://www.bing.com/ck/a?!&&p=1081c5106f8d5e08fe11b79afbc7a3cf679efaddff7ed983cb3bb8ff21c67520JmltdHM9MTc1ODU4NTYwMA&ptn=3&ver=2&hsh=4&fclid=11f78f6f-4226-66ca-395a-9afb432267e8&psq=how+do+transformers+work+ML&u=a1aHR0cHM6Ly9odWdnaW5nZmFjZS5jby9sZWFybi9sbG0tY291cnNlL2VuL2NoYXB0ZXIxLzQ) \n  - Datacamp: [Transformers Explained Visually](https://www.datacamp.com/tutorial/transformer-neural-network).  \n\n- **Mixture of Experts (MoE)**  \n  - Shazeer, N., et al. *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.* arXiv:1701.06538 (2017).\n  - How do MOE's work: [How Do MOE's work](https://www.bing.com/ck/a?!&&p=bbe74d210d7863f6342b5f225a4c312ab7da83c61e256cc9dfc35e010f071c8eJmltdHM9MTc1ODU4NTYwMA&ptn=3&ver=2&hsh=4&fclid=11f78f6f-4226-66ca-395a-9afb432267e8&psq=How+do+MOE%27s+work+ML&u=a1aHR0cHM6Ly9tZWRpdW0uY29tL0BtbmUvZXhwbGFpbmluZy10aGUtbWl4dHVyZS1vZi1leHBlcnRzLW1vZS1hcmNoaXRlY3R1cmUtaW4tc2ltcGxlLXRlcm1zLTg1ZGU5ZDE5ZWE3Mw)\n  - A Visual Guide to Mixture of Experts (MoE): [A Visual Guide to MOE's](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)\n  - Hugging Face: [Visual Guide to Mixture of Experts (MoE)](https://huggingface.co/blog/moe).  \n\n---\n\n## Models Implemented in V1\n\n- **Feedforward Neural Network (FNN)** – fully customizable  \n- **Convolutional Neural Networks (CNNs)** – base + variants  \n- **Transformers** – vanilla encoder-decoder, extendable for variants  \n- **Mixture of Experts (MoE)** – router + expert visualization  \n- *(Stretch goals, left for community)*: Autoencoder, Variational Autoencoder (VAE)  \n\n---\n\n## Lessons Learned\n\nThis project wasn’t about breaking new ground in ML theory — it was about **testing and solidifying my own understanding**.  \n\nKey takeaways:  \n- **Visualization matters.** Most ML work is hidden in math or code. Seeing the *flow of data* across blocks and layers helps build intuition and makes architectures less abstract.  \n- **Refresher on fundamentals.** Re-implementing CNNs, Transformers, and MoEs from scratch was a great way to confirm I actually understood them at a structural level.  \n- **Educational potential.** Visualizations combined with citations allow learners to both *see* the architecture and *read deeper* from the sources.  \n- **Scope discipline.** By keeping V1 focused (FNN, CNN, Transformer, MoE + quality-of-life features like light/dark mode), the project reached a natural “feature complete” state instead of drifting endlessly.  \n\n---\n\n## Closing Note\n\nNexaVisualize is **feature complete for me**.  \n- Community contributions are welcome via PRs.  \n- If you’d like to extend it (e.g., add ResNets, LSTMs, or VAEs), the modular base classes are designed to be extendable.  \n\nFor me, this project is done. For the community, it’s a playground.  \n";
